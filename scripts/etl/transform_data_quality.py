from __future__ import annotations
import os
from typing import List, Tuple
from datetime import datetime, timezone

import pandas as pd
import psycopg2
from psycopg2.extras import execute_values
from sqlalchemy import create_engine

from scripts.utils.env_loader import load_env_file

# â”€â”€ Chargement ENV (.env en Airflow, tests/.env_test si ENV_FILE dÃ©fini)
load_env_file()

# â”€â”€ Connexion Postgres
DB_CONN_INFO = {
    "host": os.getenv("POSTGRES_HOST"),
    "database": os.getenv("POSTGRES_DB"),
    "user": os.getenv("POSTGRES_USER"),
    "password": os.getenv("POSTGRES_PASSWORD"),
    "port": os.getenv("POSTGRES_PORT"),
}

# Nom de table (prend dâ€™abord TABLE_AQI_TRANSFORM, sinon fallback)
TABLE_TRANS = os.getenv("TABLE_AQI_TRANSFORM") or os.getenv("TABLE_TRANS") or "aqi_transform"

# Colonnes Â« transform Â» attendues
TRANSFORM_COLUMNS: List[str] = [
    "time", "city", "latitude", "longitude",
    "european_aqi", "pm2_5", "pm10",
    "carbon_monoxide", "sulphur_dioxide", "uv_index",
]

NUMERIC_COLS: List[str] = [
    "latitude", "longitude", "european_aqi", "pm2_5", "pm10",
    "carbon_monoxide", "sulphur_dioxide", "uv_index",
]

CRITICAL_COLS: List[str] = ["time", "city", "latitude", "longitude"]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Chargement des donnÃ©es
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_data() -> pd.DataFrame:
    try:
        url = (
            f"postgresql+psycopg2://{DB_CONN_INFO['user']}:"
            f"{DB_CONN_INFO['password']}@{DB_CONN_INFO['host']}:"
            f"{DB_CONN_INFO['port']}/{DB_CONN_INFO['database']}"
        )
        engine = create_engine(url)
        return pd.read_sql(f'SELECT * FROM {TABLE_TRANS};', con=engine)
    except Exception as e:
        print(f"âŒ Erreur lors de la lecture de la base : {e}")
        return pd.DataFrame()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Checks de qualitÃ© (impression console) â†’ True si anomalies
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def data_quality_checks(df: pd.DataFrame) -> bool:
    anomalies = False
    now_utc = pd.Timestamp.now(tz="UTC")

    print("\n1) SchÃ©ma / dtypes :")
    print(df.dtypes)

    print("\n2) Valeurs manquantes par colonne :")
    missing = df.isna().sum()
    print(missing)
    if missing.any():
        print("âŒ Anomalie : valeurs manquantes dÃ©tectÃ©es")
        anomalies = True

    print("\n3) Doublons (sur city,time) :")
    key_dups = df.duplicated(subset=["city", "time"]).sum() if {"city", "time"}.issubset(df.columns) else 0
    print(f"Doublons city+time : {key_dups}")
    if key_dups > 0:
        print("âŒ Anomalie : doublons sur (city,time)")
        anomalies = True

    print("\n4) Statistiques descriptives (numÃ©riques) :")
    try:
        print(df[NUMERIC_COLS].describe())
    except Exception:
        pass

    print("\n5) RÃ¨gles de domaine :")
    # time futur
    if "time" in df:
        # conversion douce pour le check
        time_parsed = pd.to_datetime(df["time"], errors="coerce", utc=True)
        future_cnt = (time_parsed > now_utc).sum()
        print(f"   time dans le futur : {future_cnt}")
        if future_cnt > 0:
            print("âŒ Anomalie : dates futures")
            anomalies = True

    # lat/lon plausibles
    lat_bad = (~df.get("latitude", pd.Series(dtype=float)).between(-90, 90)).sum() if "latitude" in df else 0
    lon_bad = (~df.get("longitude", pd.Series(dtype=float)).between(-180, 180)).sum() if "longitude" in df else 0
    print(f"   latitude hors [-90,90] : {lat_bad}")
    print(f"   longitude hors [-180,180] : {lon_bad}")
    if lat_bad or lon_bad:
        print("âŒ Anomalie : coordonnÃ©es invalides")
        anomalies = True

    # Polluants / indices non nÃ©gatifs
    negatives = {}
    for col in ["european_aqi", "pm2_5", "pm10", "carbon_monoxide", "sulphur_dioxide", "uv_index"]:
        if col in df:
            col_num = pd.to_numeric(df[col], errors="coerce")
            negatives[col] = (col_num < 0).sum()
    if negatives:
        print("   valeurs nÃ©gatives (par col) :", negatives)
        if any(v > 0 for v in negatives.values()):
            print("âŒ Anomalie : valeurs nÃ©gatives dÃ©tectÃ©es")
            anomalies = True

    return anomalies


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Nettoyage conservateur
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    print("\nğŸ§¹ Nettoyage des donnÃ©esâ€¦")
    df = df.copy()

    # Garder les colonnes connues si prÃ©sentes
    cols_present = [c for c in TRANSFORM_COLUMNS if c in df.columns]
    df = df[cols_present]

    # Types / conversions
    if "time" in df.columns:
        df["time"] = pd.to_datetime(df["time"], errors="coerce", utc=True)

    for c in NUMERIC_COLS:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    # Lignes critiques complÃ¨tes
    df = df.dropna(subset=[c for c in CRITICAL_COLS if c in df.columns])

    # Domain rules
    if "latitude" in df.columns:
        df = df[df["latitude"].between(-90, 90)]
    if "longitude" in df.columns:
        df = df[df["longitude"].between(-180, 180)]
    if "time" in df.columns:
        df = df[df["time"] <= pd.Timestamp.now(tz="UTC")]

    for c in ["european_aqi", "pm2_5", "pm10", "carbon_monoxide", "sulphur_dioxide", "uv_index"]:
        if c in df.columns:
            df[c] = df[c].clip(lower=0)

    # Remplissages prudents sur numÃ©riques manquants (mÃ©diane)
    for c in [col for col in NUMERIC_COLS if col in df.columns]:
        if df[c].isna().any():
            df[c] = df[c].fillna(df[c].median())

    # DÃ©duplication clÃ© (city,time)
    if {"city", "time"}.issubset(df.columns):
        df = df.sort_values("time").drop_duplicates(subset=["city", "time"], keep="last")

    # RÃ©-ordonne / complÃ¨te les colonnes attendues
    for c in TRANSFORM_COLUMNS:
        if c not in df.columns:
            df[c] = pd.NA
    df = df[TRANSFORM_COLUMNS]

    print("âœ… Nettoyage terminÃ©.")
    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RÃ©Ã©criture table (TRUNCATE + INSERT batch)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def update_data(df: pd.DataFrame) -> None:
    print(f"\nğŸ—„ï¸  RÃ©Ã©criture de {TABLE_TRANS}â€¦")
    # psycopg2 gÃ¨re trÃ¨s bien les datetime tz-aware ; sinon convertis en str ISO
    # df["time"] = df["time"].astype(str)

    records = []
    for row in df.itertuples(index=False):
        tup = tuple(
            (val.to_pydatetime() if isinstance(val, pd.Timestamp) else val)
            for val in row
        )
        records.append(tup)

    with psycopg2.connect(**DB_CONN_INFO) as conn:
        with conn.cursor() as cur:
            cur.execute(f"TRUNCATE TABLE {TABLE_TRANS};")
            placeholders = "(" + ",".join(["%s"] * len(TRANSFORM_COLUMNS)) + ")"
            sql = f"INSERT INTO {TABLE_TRANS} ({', '.join(TRANSFORM_COLUMNS)}) VALUES %s"
            execute_values(cur, sql, records, template=placeholders)
        conn.commit()
    print(f"âœ… {len(records)} lignes Ã©crites dans {TABLE_TRANS}.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Orchestration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def quality_data():
    df = load_data()
    if df.empty:
        print(f"âŒ Aucune donnÃ©e trouvÃ©e dans `{TABLE_TRANS}`.")
        return

    print(f"ğŸ” ContrÃ´le qualitÃ© sur {len(df)} lignesâ€¦")
    anomalies = data_quality_checks(df)

    if anomalies:
        print("âš ï¸  Anomalies dÃ©tectÃ©es. Nettoyage en coursâ€¦")
        cleaned_df = clean_data(df)
        print(f"ğŸ“‰ Lignes aprÃ¨s nettoyage : {len(cleaned_df)}")
        update_data(cleaned_df)
    else:
        print("âœ… DonnÃ©es conformes. Aucun nettoyage requis.")

if __name__ == "__main__":
    quality_data()
